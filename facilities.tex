\chapter{Equipment, Facilities \& Other Resources}

%% EQUIPMENT, FACILITIES & OTHER RESOURCES: This information is used to assess
%% the scale and quality of non-budgetary resources available to the program.  At
%% a high level, identify the equipment, facilities, and other resources utilized
%% by the program during the performance period.  If appropriate, indicate
%% capabilities, location, and extent of availability to the base program.
%% Describe only those resources that were directly applicable to the research
%% performed.

This project and its predecessors rely only on computational resources, both
at the desktop scale for software development, and at the institutional scale
for large simulations. The University of Wisconsin-Madison (UW-Madison) campus
is an excellent match for meeting the computational needs of this
project. Existing UW-Madison technology infrastructure supported by the
\gls{CHTC} can be readily leveraged, including CPU capacity, network
connectivity, storage availability, and middleware connectivity. But perhaps
most important, the UW-Madison has significant staff experience and core
competency in deploying, managing, and using computational technology. The
UW-Madison's \gls{ACI} has invested in the \gls{CHTC} as the primary provider
of shared large-scale computing infrastructure to campus researchers, and all
standard \gls{CHTC} services are provided free-of-charge to campus
researchers, with rarely-necessary options for researcher purchases of
dedicated hardware (determined on a case-by-case basis).

For \gls{HTC}x capability, UW-Madison maintains many compute clusters across
campus, which are managed via software developed by the UW-Madison's HTCondor
Project distributed computing research group; therefore, these clusters are
linked together to share resources via widely adopted distributed computing
technologies. Together these clusters represent roughly 30,000 CPU cores in
support of research. In 2017, \gls{CHTC} supported more than 380 million CPU
hours of computing work for campus researchers. Temporary file space for large
individual files can support up to hundreds of terabytes of total working
data. For single computing runs needing significant memory on a single server,
the \gls{CHTC} maintains several multi-core servers terabytes of
memory. Should these resources not be sufficient for your project, \gls{CHTC}
users can also engage computing resources from the \gls{OSG}, also at no cost
to researchers. \gls{OSG} is an expanding, NSF-funded alliance of more than
120 universities, national laboratories, scientific collaborations, and
software developers, and \gls{CHTC} users have automatic access to \gls{OSG}'s
considerable computing and storage resources. Individual users of our
\gls{HTC} system, including \gls{OSG} capacity, can frequently obtain in
excess of 200,000 CPU hours per day.

For \gls{HPC} capability, the \gls{CHTC} also maintains a shared-use cluster
of roughly 7000 tightly coupled cores. Compute nodes have 16 or 20 cores,
each, and 64 or 128 GB RAM, with access to a shared file system and resources
managed via the open-source software, SLURM. This cluster, deployed in June
2013 and expanded significantly in summer 2014, delivered more than 50 million
CPU hours in 2017. The current configuration of this cluster is the result of
intimate collaborations with UW-Madison's \gls{ACI} and fellow computing
centers at other campuses through the NSF-funded ACI-REF project.

Between the \gls{ACI}, \gls{CHTC}, and aforementioned HTCondor Project,
UW-Madison is home to over 20 full-time staff with a proven track record of
making compute middleware work for scientists. Far beyond just being familiar
with the deployment and use of such software, UW staff has been intimately
involved in its design and implementation. Furthermore, the \gls{CHTC}
provides consulting for the development of robust \gls{HTC} and \gls{HPC}
research methods, support for grant proposal development, and a variety of
formal and informal training opportunities for users of \gls{CHTC} resources.

This project uses a combination of \gls{HTC} and \gls{HPC} resources for
completing large scale simulations of radiation transport in fusion energy
systems.
